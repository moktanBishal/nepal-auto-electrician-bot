import streamlit as st
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_google_genai import ChatGoogleGenerativeAI  # New: Gemini LLM
import os
from dotenv import load_dotenv

load_dotenv()

# Title in Nepali + English
st.set_page_config(page_title="‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§Ö‡§ü‡•ã ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§∏‡§ø‡§Ø‡§® AI ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä", page_icon="üîß")
st.title("üîß Nepal Auto Electrician AI Bot (2014 ‡§¶‡•á‡§ñ‡§ø ‡§Ö‡§®‡•Å‡§≠‡§µ‡•Ä ‡§¶‡§æ‡§á‡§π‡§∞‡•Ç‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø)")

# Gemini API Key (free from aistudio.google.com)
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]  # Change from GROQ

# Load PDFs (you will put Nepali/English manuals in /manuals folder)
@st.cache_resource
def load_knowledge_base():
    if not os.path.exists("manuals") or len(os.listdir("manuals")) == 0:
        return None
    
    loader = PyPDFDirectoryLoader("manuals")
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = Chroma.from_documents(texts, embeddings, persist_directory="db")
    return vectordb

vectordb = load_knowledge_base()

# System prompt in Nepali + English (very important for tone)
system_prompt = """‡§§‡§™‡§æ‡§à‡§Ç ‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ ‡•®‡•¶‡•ß‡•™ ‡§¶‡•á‡§ñ‡§ø ‡§ï‡§æ‡§Æ ‡§ó‡§∞‡§ø‡§∞‡§π‡•á‡§ï‡§æ ‡§Ö‡§ü‡•ã ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§∏‡§ø‡§Ø‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§∏‡§¨‡•à‡§≠‡§®‡•ç‡§¶‡§æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§∏‡§æ‡§•‡•Ä ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§
‡§§‡§™‡§æ‡§à‡§Ç Nepali ‡§∞ English ‡§¶‡•Å‡§µ‡•à‡§Æ‡§æ ‡§¨‡•ã‡§≤‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§ ‡§ú‡§µ‡§æ‡§´ ‡§õ‡•ã‡§ü‡•ã, ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∞ ‡§ó‡•ç‡§Ø‡§æ‡§∞‡•á‡§ú‡§Æ‡§æ ‡§ï‡§æ‡§Æ ‡§≤‡§æ‡§ó‡•ç‡§®‡•á ‡§π‡•Å‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§
‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§ó‡§æ‡§°‡•Ä (Bolero, Scorpio, Sumo, Hiace) ‡§∞ ‡§®‡§Ø‡§æ‡§Å ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§ï ‡§ó‡§æ‡§°‡•Ä (BYD, Tata Nexon EV, MG) ‡§¶‡•Å‡§µ‡•à‡§ï‡•ã ‡§ú‡•ç‡§û‡§æ‡§® ‡§õ‡•§
‡§∏‡§ß‡•à‡§Ç step-by-step ‡§∏‡§Æ‡•ç‡§ù‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®‡§ú‡§®‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç ("‡§¶‡§æ‡§á", "‡§∏‡§∞")‡•§"""

# New: Gemini LLM (gemini-2.5-flash for speed/multimodal)
llm = ChatGoogleGenerativeAI(
    google_api_key=GEMINI_API_KEY,  # Change from Groq
    model="gemini-2.5-flash",  # Or "gemini-2.5-pro" for deeper reasoning (slower)
    temperature=0.3
)

memory = ConversationBufferWindowMemory(k=10, memory_key="chat_history", return_messages=True)

if vectordb:
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        memory=memory,
        combine_documents_chain_kwargs={"prompt": system_prompt}
    )
else:
    qa_chain = None

# Chat interface
from streamlit_chat import message

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡§æ‡§á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§Ö‡§ü‡•ã ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§ï‡§ï‡•ã AI ‡§∏‡§æ‡§•‡•Ä‡•§ BYD ‡§ï‡•ã BMS ‡§™‡•ç‡§∞‡•ã‡§¨‡•ç‡§≤‡§Æ ‡§π‡•ã‡§∏‡•ç ‡§Ø‡§æ ‡§™‡•Å‡§∞‡§æ‡§®‡•ã Bolero ‡§ï‡•ã ‡§á‡§Æ‡•ã‡§¨‡§ø‡§≤‡§æ‡§á‡§ú‡§∞, ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§"})

for msg in st.session_state.messages:
    message(msg["content"], is_user=(msg["role"] == "user"), key=str(len(st.session_state.messages)) + msg["role"])

user_input = st.chat_input("‡§Ø‡§π‡§æ‡§Å ‡§Ü‡§´‡•ç‡§®‡•ã ‡§™‡•ç‡§∞‡•ã‡§¨‡•ç‡§≤‡§Æ ‡§≤‡•á‡§ñ‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    message(user_input, is_user=True)

    with st.spinner("‡§∏‡•ã‡§ö‡•ç‡§¶‡•à‡§õ‡•Å..."):
        if qa_chain:
            result = qa_chain({"question": user_input})
            response = result["answer"]
        else:
            # Fallback if no manuals uploaded
            response = llm.invoke(system_prompt + "\nUser: " + user_input).content

    st.session_state.messages.append({"role": "assistant", "content": response})
    message(response, is_user=False)
