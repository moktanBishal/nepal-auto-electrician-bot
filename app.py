import streamlit as st
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS  # Using FAISS for speed
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from dotenv import load_dotenv

load_dotenv()

# Title in Nepali + English
st.set_page_config(page_title="‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§Ö‡§ü‡•ã ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§∏‡§ø‡§Ø‡§® AI ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä", page_icon="üîß", layout="wide")
st.title("üîß Nepal Auto Electrician AI Bot (2014 ‡§¶‡•á‡§ñ‡§ø ‡§Ö‡§®‡•Å‡§≠‡§µ‡•Ä ‡§¶‡§æ‡§á‡§π‡§∞‡•Ç‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø)")

# Gemini API Key
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")  # Use os.getenv for Render
if not GEMINI_API_KEY:
    st.error("‚ùå GEMINI_API_KEY not set! Add it in Render env vars.")
    st.stop()

# Load PDFs (put in /manuals folder on GitHub)
@st.cache_resource
def load_knowledge_base():
    if not os.path.exists("manuals") or len(os.listdir("manuals")) == 0:
        st.info("üìö No manuals found‚Äîusing pure AI knowledge. Add PDFs to /manuals folder.")
        return None
    
    loader = PyPDFDirectoryLoader("manuals")
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(docs)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(texts, embeddings)
    return vectordb

vectordb = load_knowledge_base()

# System prompt
system_prompt = """‡§§‡§™‡§æ‡§à‡§Ç ‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ ‡•®‡•¶‡•ß‡•™ ‡§¶‡•á‡§ñ‡§ø ‡§ï‡§æ‡§Æ ‡§ó‡§∞‡§ø‡§∞‡§π‡•á‡§ï‡§æ ‡§Ö‡§ü‡•ã ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§∏‡§ø‡§Ø‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§∏‡§¨‡•à‡§≠‡§®‡•ç‡§¶‡§æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§∏‡§æ‡§•‡•Ä ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§
‡§§‡§™‡§æ‡§à‡§Ç Nepali ‡§∞ English ‡§¶‡•Å‡§µ‡•à‡§Æ‡§æ ‡§¨‡•ã‡§≤‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§ ‡§ú‡§µ‡§æ‡§´ ‡§õ‡•ã‡§ü‡•ã, ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∞ ‡§ó‡•ç‡§Ø‡§æ‡§∞‡•á‡§ú‡§Æ‡§æ ‡§ï‡§æ‡§Æ ‡§≤‡§æ‡§ó‡•ç‡§®‡•á ‡§π‡•Å‡§®‡•Å‡§™‡§∞‡•ç‡§õ‡•§
‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§ó‡§æ‡§°‡•Ä (Bolero, Scorpio, Sumo, Hiace) ‡§∞ ‡§®‡§Ø‡§æ‡§Å ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§ï ‡§ó‡§æ‡§°‡•Ä (BYD, Tata Nexon EV, MG) ‡§¶‡•Å‡§µ‡•à‡§ï‡•ã ‡§ú‡•ç‡§û‡§æ‡§® ‡§õ‡•§
‡§∏‡§ß‡•à‡§Ç step-by-step ‡§∏‡§Æ‡•ç‡§ù‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®‡§ú‡§®‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç ("‡§¶‡§æ‡§á", "‡§∏‡§∞")‡•§"""

# Gemini LLM
llm = ChatGoogleGenerativeAI(
    google_api_key=GEMINI_API_KEY,
    model="gemini-2.5-flash",
    temperature=0.3
)

memory = ConversationBufferWindowMemory(k=10, memory_key="chat_history", return_messages=True)

if vectordb:
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        memory=memory,
        combine_documents_chain_kwargs={"prompt": system_prompt}
    )
else:
    qa_chain = None

# Native Streamlit Chat (no streamlit-chat needed!)
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡§æ‡§á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§Ö‡§ü‡•ã ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡§ø‡§ï‡§ï‡•ã AI ‡§∏‡§æ‡§•‡•Ä‡•§ BYD ‡§ï‡•ã BMS ‡§™‡•ç‡§∞‡•ã‡§¨‡•ç‡§≤‡§Æ ‡§π‡•ã‡§∏‡•ç ‡§Ø‡§æ ‡§™‡•Å‡§∞‡§æ‡§®‡•ã Bolero ‡§ï‡•ã ‡§á‡§Æ‡•ã‡§¨‡§ø‡§≤‡§æ‡§á‡§ú‡§∞, ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§"}]

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("‡§Ø‡§π‡§æ‡§Å ‡§Ü‡§´‡•ç‡§®‡•ã ‡§™‡•ç‡§∞‡•ã‡§¨‡•ç‡§≤‡§Æ ‡§≤‡•á‡§ñ‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("‡§∏‡•ã‡§ö‡•ç‡§¶‡•à‡§õ‡•Å..."):
            if qa_chain:
                result = qa_chain({"question": prompt})
                response = result["answer"]
            else:
                # Fallback
                response = llm.invoke(system_prompt + "\nUser: " + prompt).content
            st.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
